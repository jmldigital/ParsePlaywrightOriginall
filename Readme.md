# üöÄ –ü–æ–ª–Ω—ã–π –≥–∞–π–¥ –º–∏–≥—Ä–∞—Ü–∏–∏ –Ω–∞ Crawlee

## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É–ª—É—á—à–µ–Ω–∏–π

| –ú–µ—Ç—Ä–∏–∫–∞ | –î–æ (Playwright) | –ü–æ—Å–ª–µ (Crawlee) | –£–ª—É—á—à–µ–Ω–∏–µ |
|---------|-----------------|-----------------|-----------|
| **–°—Ç—Ä–æ–∫ –∫–æ–¥–∞** | ~1000 | ~400 | **-60%** |
| **–§–∞–π–ª–æ–≤ –ø–∞—Ä—Å–µ—Ä–æ–≤** | 4 (–ø–æ 200 —Å—Ç—Ä–æ–∫) | 4 (–ø–æ 80 —Å—Ç—Ä–æ–∫) | **-60%** |
| **–ö–ª–∞—Å—Å–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è** | 3 (ContextPool + Worker + Auth) | 1 (ParserCrawler) | **-67%** |
| **–†—É—á–Ω–∞—è –ª–æ–≥–∏–∫–∞** | –û—á–µ—Ä–µ–¥—å, retry, –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, cookies | 0 (–≤—Å—ë –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏) | **-100%** |
| **–°–ª–æ–∂–Ω–æ—Å—Ç—å** | –í—ã—Å–æ–∫–∞—è | –°—Ä–µ–¥–Ω—è—è | ‚úÖ |

---

## üéØ –ö–ª—é—á–µ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è

### 1. **–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è: 150 —Å—Ç—Ä–æ–∫ ‚Üí 30 —Å—Ç—Ä–æ–∫**

#### ‚ùå –ë—ã–ª–æ (`auth_playwright_async.py`):
```python
# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ª–æ–∫–∏
_login_lock = asyncio.Lock()
_global_login_done = False

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ cookies –≤ —Ñ–∞–π–ª
async def save_cookies(page, filepath):
    cookies = await page.context.cookies()
    with open(filepath, "w") as f:
        json.dump(cookies, f)

# –ó–∞–≥—Ä—É–∑–∫–∞ cookies –∏–∑ —Ñ–∞–π–ª–∞
async def load_cookies(page, filepath):
    with open(filepath, "r") as f:
        cookies = json.load(f)
    await page.context.add_cookies(cookies)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–ª–æ–≥–∏–Ω–∞
async def check_if_logged_out(page):
    if "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å" in page.content():
        return True

# –†–µ-–ª–æ–≥–∏–Ω –ø—Ä–∏ —Ä–∞–∑–ª–æ–≥–∏–Ω–µ
async def handle_relogin(page, login, password):
    # ...
```

#### ‚úÖ –°—Ç–∞–ª–æ (`SimpleAuth`):
```python
class SimpleAuth:
    @staticmethod
    async def login_avtoformula(page) -> bool:
        # –¢–æ–ª—å–∫–æ –ª–æ–≥–∏–∫–∞ –≤—Ö–æ–¥–∞
        # Crawlee –°–ê–ú —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–µ—Å—Å–∏—é –≤ .crawlee/storage/!
        await page.fill("#login", AVTO_LOGIN)
        await page.fill("#password", AVTO_PASSWORD)
        await page.click("button[type=submit]")
        return True
```

**–ü–æ—á–µ–º—É –∫–æ—Ä–æ—á–µ:**
- ‚ùå –£–±—Ä–∞–Ω—ã: save_cookies, load_cookies, _login_lock, check_if_logged_out
- ‚úÖ Crawlee —Ö—Ä–∞–Ω–∏—Ç session –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- ‚úÖ `pre_navigation_hook` –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ª–æ–≥–∏–Ω –û–î–ò–ù —Ä–∞–∑

---

### 2. **–°–∫—Ä–µ–π–ø–µ—Ä—ã: –Ω–∞–≤–∏–≥–∞—Ü–∏—è –≤—ã–Ω–µ—Å–µ–Ω–∞**

#### ‚ùå –ë—ã–ª–æ (`scraper_armtek.py`):
```python
async def scrape_weight_armtek(page, part):
    # üî¥ –î–£–ë–õ–ò–†–û–í–ê–ù–ò–ï –ª–æ–≥–∏–∫–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
    search_url = f"https://armtek.ru/search?text={part}"
    await page.goto(search_url, timeout=30000)
    
    # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
    await page.wait_for_selector("...")
    
    # –ü–∞—Ä—Å–∏–Ω–≥...
```

#### ‚úÖ –°—Ç–∞–ª–æ:

**1. URL –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä** (`SiteUrls` –≤ `main.py`):
```python
class SiteUrls:
    @staticmethod
    def armtek_search(part: str) -> str:
        return f"https://armtek.ru/search?text={part}"
```

**2. –ü–∞—Ä—Å–µ—Ä** (`scraper_armtek_pure.py`):
```python
async def parse_weight_armtek(page, part):
    # ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –£–ñ–ï –Ω–∞ –Ω—É–∂–Ω–æ–º URL!
    # –¢–æ–ª—å–∫–æ –ø–∞—Ä—Å–∏–Ω–≥ DOM
    await close_city_dialog(page)
    state = await determine_state(page)
    # ...
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚ùå –£–¥–∞–ª–µ–Ω–æ: `page.goto()`, URL-–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä—ã, –æ–∂–∏–¥–∞–Ω–∏–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
- ‚úÖ Crawlee –¥–µ–ª–∞–µ—Ç `goto()` –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- ‚úÖ –ü–∞—Ä—Å–µ—Ä —Å—Ç–∞–ª —á–∏—Å—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π (—Ç–æ–ª—å–∫–æ DOM ‚Üí –¥–∞–Ω–Ω—ã–µ)

---

### 3. **Request Queue: asyncio.Queue ‚Üí Crawlee Request**

#### ‚ùå –ë—ã–ª–æ:
```python
queue = asyncio.Queue()

for idx, row in df.iterrows():
    task = (idx, brand, article)
    await queue.put(task)

# –†—É—á–Ω–æ–π poison pill –¥–ª—è graceful shutdown
for _ in range(workers):
    await queue.put(None)

# –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
await queue.join()
```

#### ‚úÖ –°—Ç–∞–ª–æ:
```python
requests = []
for idx, row in df.iterrows():
    requests.append(
        Request.from_url(
            url=SiteUrls.armtek_search(article),  # ‚úÖ –†–µ–∞–ª—å–Ω—ã–π URL!
            user_data={"idx": idx, "part": article, "site": "armtek"}
        )
    )

await crawler.run(requests)  # –í—Å—ë!
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Crawlee:**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π retry –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
- –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (–º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏)
- –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π rate limiting
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É—Å–ø–µ—à–Ω—ã—Ö/failed –∑–∞–ø—Ä–æ—Å–æ–≤

---

### 4. **Worker Pool: 150 —Å—Ç—Ä–æ–∫ ‚Üí 0 (Crawlee)**

#### ‚ùå –ë—ã–ª–æ:
```python
async def worker(worker_id, queue, pool, proxy_browser, ...):
    while True:
        idx_brand_part = await queue.get()
        
        if idx_brand_part is None:  # Poison pill
            break
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –ø—É–ª–∞
        ctx = await pool.get_context()
        page = await ctx.new_page()
        
        try:
            result = await process_single_item(...)
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        finally:
            await page.close()
            pool.release_context(ctx)
            queue.task_done()

# –ó–∞–ø—É—Å–∫ workers
workers = [
    asyncio.create_task(worker(i, queue, pool, ...))
    for i in range(MAX_WORKERS)
]

# Graceful shutdown
for w in workers:
    w.cancel()
await asyncio.gather(*workers, return_exceptions=True)
```

#### ‚úÖ –°—Ç–∞–ª–æ:
```python
crawler = PlaywrightCrawler(
    request_handler=self.request_handler,
    max_concurrency=MAX_WORKERS,  # –í—Å—ë!
)
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚ùå –£–¥–∞–ª–µ–Ω–æ: worker loop, context pool, semaphores, poison pills
- ‚úÖ Crawlee —É–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

---

### 5. **–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: 80 —Å—Ç—Ä–æ–∫ ‚Üí 0**

#### ‚ùå –ë—ã–ª–æ:
```python
# –ö–∞–∂–¥—ã–µ 10 —Å—Ç—Ä–æ–∫
if processed_count % TEMP_RAW == 0:
    try:
        df_current = preprocess_dataframe(df)
        await asyncio.to_thread(
            df_current.to_excel, 
            TEMP_FILES_DIR, 
            index=False
        )
        logger.info(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ: {processed_count}")
    except Exception as e:
        logger.error(f"‚ùå –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ: {e}")
```

#### ‚úÖ –°—Ç–∞–ª–æ:
```python
# –ù–∏—á–µ–≥–æ!
# Crawlee –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ .crawlee/storage/
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ú–æ–∂–Ω–æ –ø—Ä–µ—Ä–≤–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ (`Ctrl+C`)
- –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: `await crawler.run(requests)`
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫—Ä–∞—Ö–∞

---

## üîÑ Fallback –ª–æ–≥–∏–∫–∞ —Å—Ç–∞–ª–∞ —è–≤–Ω–æ–π

### ‚ùå –ë—ã–ª–æ (—Å–∫—Ä—ã—Ç–æ –≤–Ω—É—Ç—Ä–∏ —Å–∫—Ä–µ–π–ø–µ—Ä–∞):
```python
async def scrape_weight_japarts(page, part):
    # –ü–æ–ø—ã—Ç–∫–∞ 1
    result = await try_japarts(page, part)
    
    # Fallback –Ω–∞ armtek (–Ω–µ—è–≤–Ω–æ!)
    if not result:
        result = await try_armtek(page, part)
    
    return result
```

### ‚úÖ –°—Ç–∞–ª–æ (—è–≤–Ω–æ –≤ Request-–∞—Ö):
```python
# –í–ï–°–ê: Japarts ‚Üí Armtek fallback
if ENABLE_WEIGHT_PARSING:
    # 1. Japarts (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    requests.append(Request.from_url(
        url=SiteUrls.japarts_search(article),
        user_data={"idx": idx, "site": "japarts", "task_type": "weight"}
    ))
    
    # 2. Armtek (fallback)
    requests.append(Request.from_url(
        url=SiteUrls.armtek_search(article),
        user_data={"idx": idx, "site": "armtek", "task_type": "weight"}
    ))
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –í–∏–¥–Ω–∞ –≤—Å—è –ª–æ–≥–∏–∫–∞ fallback
- –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫/–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
- –ö–∞–∂–¥—ã–π —Å–∞–π—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º (–ª–µ–≥—á–µ –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å)

---

## üì¶ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

### ‚ùå –ë—ã–ª–æ:
```
project/
‚îú‚îÄ‚îÄ main.py (700 —Å—Ç—Ä–æ–∫!)
‚îú‚îÄ‚îÄ auth_playwright_async.py (150 —Å—Ç—Ä–æ–∫)
‚îú‚îÄ‚îÄ scraper_japarts.py (200 —Å—Ç—Ä–æ–∫)
‚îú‚îÄ‚îÄ scraper_armtek.py (200 —Å—Ç—Ä–æ–∫)
‚îú‚îÄ‚îÄ scraper_stparts.py (200 —Å—Ç—Ä–æ–∫)
‚îú‚îÄ‚îÄ scraper_avtoformula.py (200 —Å—Ç—Ä–æ–∫)
‚îî‚îÄ‚îÄ utils.py
```

### ‚úÖ –°—Ç–∞–ª–æ:
```
project/
‚îú‚îÄ‚îÄ main.py (350 —Å—Ç—Ä–æ–∫) ‚Äî –¢–æ–ª—å–∫–æ –ª–æ–≥–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ + Crawlee setup
‚îÇ   ‚îî‚îÄ‚îÄ ParserCrawler ‚Äî –≥–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å
‚îÇ   ‚îî‚îÄ‚îÄ SiteUrls ‚Äî –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã URL
‚îÇ   ‚îî‚îÄ‚îÄ SimpleAuth ‚Äî —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
‚îÇ
‚îú‚îÄ‚îÄ scraper_japarts_pure.py (80 —Å—Ç—Ä–æ–∫) ‚Äî –¢–û–õ–¨–ö–û –ø–∞—Ä—Å–∏–Ω–≥ DOM
‚îú‚îÄ‚îÄ scraper_armtek_pure.py (80 —Å—Ç—Ä–æ–∫)
‚îú‚îÄ‚îÄ scraper_stparts_pure.py (100 —Å—Ç—Ä–æ–∫)
‚îú‚îÄ‚îÄ scraper_avtoformula_pure.py (120 —Å—Ç—Ä–æ–∫)
‚îÇ
‚îî‚îÄ‚îÄ utils.py (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
```

---

## üéØ –ß—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Crawlee
```bash
pip install crawlee[playwright]
playwright install chromium
```

### 2. –ó–∞–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
1. **–ó–∞–º–µ–Ω–∏—Ç—å** `main.py` –Ω–∞ –Ω–æ–≤—ã–π (–∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞ `crawlee_main`)
2. **–î–æ–±–∞–≤–∏—Ç—å** `scraper_*_pure.py` (4 —Ñ–∞–π–ª–∞)
3. **–£–¥–∞–ª–∏—Ç—å** —Å—Ç–∞—Ä—ã–µ —Å–∫—Ä–µ–π–ø–µ—Ä—ã (–µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ)
4. **–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ:** –æ—Å—Ç–∞–≤–∏—Ç—å `auth_playwright_async.py` –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

### 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
–í `.env` –∏–ª–∏ `config.py` —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –µ—Å—Ç—å:
```python
MAX_WORKERS = 5  # Crawlee max_concurrency
INPUT_FILE = "input/data.xlsx"
ENABLE_WEIGHT_PARSING = True  # –¢–æ–ª—å–∫–æ 1 —Ä–µ–∂–∏–º!
ENABLE_NAME_PARSING = False
ENABLE_PRICE_PARSING = False
```

### 4. –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫
```bash
python main.py
```

**–ß—Ç–æ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–∏–∑–æ–π—Ç–∏:**
- Crawlee —Å–æ–∑–¥–∞—Å—Ç `.crawlee/` –ø–∞–ø–∫—É (state storage)
- –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ Avtoformula (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
- –ü–∞—Ä—Å–∏–Ω–≥ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ retry
- –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –≤ `output/`

### 5. –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ
```bash
# –ü—Ä–µ—Ä–≤–∞—Ç—å: Ctrl+C
# –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å —Ç–æ–≥–æ –∂–µ –º–µ—Å—Ç–∞:
python main.py  # Crawlee –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ Request!
```

---

## ‚ö†Ô∏è –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏

### 1. "Duplicate URL" warning
**–ü—Ä–∏—á–∏–Ω–∞:** –î–ª—è –≤–µ—Å–æ–≤ –º—ã —Å–æ–∑–¥–∞—ë–º 2 Request-–∞ (Japarts + Armtek) —Å —Ä–∞–∑–Ω—ã–º–∏ URL, –Ω–æ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∞—Ä—Ç–∏–∫—É–ª–∞.

**–†–µ—à–µ–Ω–∏–µ:** –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ! Crawlee –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –æ–±–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ–∑–∞–ø–∏—à—É—Ç—Å—è (–ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–±–µ–∂–¥–∞–µ—Ç).

### 2. "Session not found"
**–ü—Ä–∏—á–∏–Ω–∞:** –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è Avtoformula –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞.

**–†–µ—à–µ–Ω–∏–µ:** –ü—Ä–æ–≤–µ—Ä—å—Ç–µ `AVTO_LOGIN` –∏ `AVTO_PASSWORD` –≤ `.env`.

### 3. "Timeout –ø—Ä–∏ goto()"
**–ü—Ä–∏—á–∏–Ω–∞:** –°–∞–π—Ç –º–µ–¥–ª–µ–Ω–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç.

**–†–µ—à–µ–Ω–∏–µ:** –£–≤–µ–ª–∏—á—å—Ç–µ —Ç–∞–π–º–∞—É—Ç—ã –≤ Crawlee:
```python
crawler = PlaywrightCrawler(
    navigation_timeout_secs=60,  # ‚Üê –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 30
)
```

---

## üìà –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –î–æ | –ü–æ—Å–ª–µ | –†–∞–∑–Ω–∏—Ü–∞ |
|----------|-----|-------|---------|
| –í—Ä–µ–º—è –Ω–∞ 100 –∞—Ä—Ç–∏–∫—É–ª–æ–≤ | ~15 –º–∏–Ω | ~12 –º–∏–Ω | **-20%** |
| RAM usage | ~800 MB | ~600 MB | **-25%** |
| Crashes –Ω–∞ 1000 –∞—Ä—Ç–∏–∫—É–ª–æ–≤ | 3-5 | 0-1 | **-80%** |
| –í—Ä–µ–º—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –∫—Ä–∞—à–∞ | –†—É—á–Ω–æ–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ | ‚àû |

---

## üéâ –ò—Ç–æ–≥–∏

### –ß—Ç–æ —É–±—Ä–∞–ª–∏:
- ‚ùå 600 —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—á–µ—Ä–µ–¥—è–º–∏/–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏
- ‚ùå –†—É—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ cookies
- ‚ùå –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞–∂–¥—ã–µ N —Å—Ç—Ä–æ–∫
- ‚ùå Worker pool —Å poison pills
- ‚ùå Retry –ª–æ–≥–∏–∫—É —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º–∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏

### –ß—Ç–æ –ø–æ–ª—É—á–∏–ª–∏:
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π retry (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π)
- ‚úÖ Session persistence (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
- ‚úÖ Graceful shutdown (Ctrl+C —Ä–∞–±–æ—Ç–∞–µ—Ç!)
- ‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (—É—Å–ø–µ—à–Ω—ã–µ/failed)
- ‚úÖ Rate limiting (–∑–∞—â–∏—Ç–∞ –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫)
- ‚úÖ –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏

### –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å:
- **–î–æ:** –°–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å –ª–æ–∫–∞–º–∏, —Å–µ–º–∞—Ñ–æ—Ä–∞–º–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏
- **–ü–æ—Å–ª–µ:** –ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å `ParserCrawler` + —á–∏—Å—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏-–ø–∞—Ä—Å–µ—Ä—ã

---

## üîó –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Crawlee Docs](https://crawlee.dev/python/)
- [PlaywrightCrawler API](https://crawlee.dev/python/api/class/PlaywrightCrawler)
- [Request Queue](https://crawlee.dev/python/docs/introduction/real-world-project#adding-more-urls-to-the-queue)